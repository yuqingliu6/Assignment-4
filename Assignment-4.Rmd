---
title: "Assignment-4"
date: "2024-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r message=FALSE}
library(readr)
library(dplyr)
library(Amelia)
library(factoextra)
library(cluster)
library(caret)
```


### Part I: Implementing a Simple Prediction Pipeline

1. Perform basic data cleaning. Note which features are continuous, which are categorical and ensure they are being stored that way in your R dataset (That is, if categorical variables have been read-in as continuous variables, convert them to factors)

```{r cleaning}
# path where datasets are stored as working directory
setwd("/Users/apple/Desktop/8451 Machine Learning for Epi/Assignment-4")

# read dataset
data <- read_csv("./class4_p1.csv")

# rename the variable names to make them more easy to understand
var.names <- c("id","hypertension","diabetes","asthma","bmi","tobacco","alcohol","physical_activity_total_minutes","physical_activity_days","physically_active","diet","agegroup","sex","hispanic","born_in_us","poverty_group","healthy_days")

colnames(data) <- var.names

# check which features are continuous and which are categorical
data |> str()

# The results from str() show all the features are numeric. However, all features except for id, bmi, physical_activity_total_minutes, physical_activity_days, and healthy_days need to be categorical. 

# convert categorical variables that have been read-in as continous variables
data <- data |> mutate_at(vars(id,hypertension,diabetes,asthma,tobacco,alcohol,
                            physically_active,diet,agegroup,sex,hispanic,born_in_us,
                            poverty_group), as.factor)

# check missing data
data |> missmap(main = "Missing values vs observed") 

# The missing map shows only 3% of the data are missing. However, we may still want to omit missing value.  

# omit missing value
data <- data |> na.omit()

# check missing data age
data |> missmap(main = "Missing values vs observed") 

# Now, there is no missing value.

# check whether there is only one id per person
data <- data |> distinct(id, .keep_all = TRUE) 

# No change of the observation's number indicates there was no duplicate in the dataset.

# remove the id variable
data$id <- NULL
```


2. Partition data into training and testing (use a 70/30 split)

```{r partition data}
# split data into training and testing
train.index = createDataPartition(data$healthy_days, p = 0.7, list = FALSE)

train = data[train.index, ]
test = data[-train.index, ]
```

3. Fit two prediction  models using  different subsets of the features in the training data. Features can overlap in the two models, but the feature sets should not be exactly the same across models. Clearly state which features were used in the two models.

I will use two linar regression models to do the machine learning task. The dependent variable in both models is the number of healthy days. Model 1 contains two independent variables which are whether or not have hypertension and number of physical active days. Model 2 contains all the features available in the dataset.

* Model 1 : healthy_days ~ hypertension + physical_activity_days
* Model 2 : healthy_days ~ hypertension + diabetes + asthma + tobacco + 
                       alcohol + physically_active + diet + agegroup + sex + 
                       hispanic + born_in_us + poverty_group 

```{r fit prediction models}
# Avoid overfitting by repeated cross-validation
control = trainControl(method="repeatedcv", number=10, repeats=10, summaryFunction=defaultSummary)

# fit prediction models
model1 = train(healthy_days ~ hypertension + physical_activity_days, data = train, method = "lm", trControl = control)
model2 = train(healthy_days ~ hypertension + diabetes + asthma + tobacco + 
                       alcohol + physically_active + diet + agegroup + sex + 
                       hispanic + born_in_us + poverty_group, data = train, method = "lm", trControl = control)
```


4. Apply both models within the test data and determine which model is the preferred prediction model using the appropriate evaluation metric(s). 

```{r}
# make predictions within test dataset
prediction1 = predict(model1, test)
prediction2 = predict(model2, test)

# evaluate the models using mean squared error
mse1 = mean((prediction1 - test$healthy_days)**2/nrow(test)) |> print()
mse2 = mean((prediction2 - test$healthy_days)**2/nrow(test)) |> print()

model1_result = bind_rows(train = prediction1, test = test$healthy_days, .id = "split")
model2_result = bind_rows(train = prediction2, test = test$healthy_days, .id = "split")
result = bind_rows(model1 = model1_result, model2 = model2_result, .id = "model")

# calculate the residual
result = result |> 
  mutate(residual = train - test)

# plot the residual
result |> 
  ggplot(aes(x = model, y = residual)) + geom_violin()
```

The mean squared error (MSE) for Model 1 (0.070) is slightly higher than that of Model 2 (0.065). Additionally, the residual plot indicates that both models have residuals centered around zero, which is a desirable outcome. Moreover, the spread of the residuals, as depicted by the range from top to bottom of the violin plot, appears similar for both models, suggesting comparable variance in their predictions. However, upon closer examination, Model 2 exhibits a slightly tighter distribution of residuals, implying more consistent predictions. Therefore, based on the MSE evaluation, Model 2 is considered the preferred prediction model.

5. Describe one setting (in 1-2 sentences) where the implementation of your final model would be useful.

The Model 2 is capable of predicting the number of days an individual reports having good physical health in a month. This prediction is derived from a comprehensive evaluation of medical conditions, lifestyle habits, and socioeconomic factors. The insights gleaned from this model can inform targeted health interventions and resource allocation strategies aimed at enhancing the overall well-being of the population in NYC.

### Part II: Conducting an Unsupervised Analysis

Using the dataset from the Group assignment Part 3 (USArrests), identify clusters using hierarchical analysis.

6. Conduct a hierarchical clustering analysis. Use a Euclidian distance measure to construct your dissimilarity matrix. Use complete linkage.

```{r}
# import dataset
data("USArrests")

# strip off the outcome and id variable
USArrests <- USArrests |> select("Murder", "Assault", "UrbanPop", "Rape")

# determine if scale is necessary
USArrests |> summarise_all(mean, na.rm = TRUE) |>
  print()

USArrests |> summarise_all(sd, na.rm = TRUE) |>
  print()

# Since the mean and standard deviation's magnitude are different for each variable. So, we decide to center and scale.

# set seed
set.seed(123)

# center and scale
set.up.preprocess = preProcess(USArrests, method = c("center", "scale"))

# output pre-processed values
transformed.vals = predict(set.up.preprocess, USArrests)

# compare the sds used to scale to the sds above to ensure they are close 
USArrests$scale

# The sds used to scale to the sds above are close.

# conduct clustering analysis using hierarchical clustering by creating dissimilarity matrix first
diss.matrix <- dist(USArrests, method = "euclidean")

# hierarchical clustering using complete linkage
clusters.hcut <- hclust(diss.matrix, method = "complete")

# plot the obtained dendrogram
plot(clusters.hcut, cex = 0.6, hang = -1)
```


7. Determine the optimal number of clusters using a clear, data-driven strategy.

*  We could determine the optimal number of cluster by looking at the Gap Statistic Graph. Since when number of clusters k = 4, it gives us the highest Gap Statistic. Therefore, the optimal number of clusters is 4. 

```{r}
# use complete method
gap_stat = clusGap(transformed.vals, FUN = hcut, hc_method="complete", K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

# use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.4 <- cutree(clusters.hcut, k=4)
table(clusters.h.4)

# set k to be 4
clusters.hcut=hcut(transformed.vals, k=4, hc_func="hclust", hc_method="complete", hc_metric="euclidian")
```

8. Describe the composition of each cluster in terms of the original input features

* Cluster 1: This cluster is characterized by above-average values in all categories except UrbanPop, which is close to the average (z-score of -0.832). It suggests states with higher crime rates but moderately smaller urban populations.

* Cluster 2: States in this cluster exhibit all features above the average, especially Assault and Rape rates. This cluster likely represents states with high crime rates and large urban populations.

* Cluster 3: Negative z-scores for Murder, Assault, and Rape indicate that these areas have lower than average rates of these crimes. The UrbanPop is slightly above average, suggesting moderately sized urban populations but high crime rates in these areas.

* Cluster 4: This cluster is characterized by below-average values for all features (Murder, Assault, UrbanPop, Rape), as indicated by the negative z-scores. States falling into this cluster tend to have lower than average crime rates and urban population sizes.

```{r}
# show the mean value of features within each cluster
# bind the cluster value to original data
input.feature.vals=cbind(transformed.vals,cluster=clusters.hcut$cluster)

# summarize the mean value of features within each cluster
input.feature.vals |> group_by(cluster) |> 
  summarise_all(mean)
```


9. Pretend that the data are from 2022 and not 1973. Describe one research question that can be addressed using the newly identified clusters. Briefly comment on any scientific or ethical considerations one should review before using these clusters for your specific question. NOTE: The clusters can be used as an exposure, an outcome or a covariate. (2-4 sentences)

* We can investigate what is relationship between the outcome urban population size and indicators for crime such as murder, assault, and rape rate. If the data are from 2022, there might be more features available such as sociodemographic data as well. So, in terms of scientific considerations, we need to consider which feature we should select and how many clusters there need to have. In terms of ethical considerations, we should check whether the data is representative for all sociodemographic group in the society to avoid a biased dataset where certain groups are neglected. 

10. Optional Repeat analysis with a different linkage method (e.g. single or average). Do the clusters change?

```{r}
# change the linkage method to single 
clusters.hcut <-hcut(USArrests, k = 4, hc_func = "hclust", hc_method = "single", hc_metric = "euclidian")

clusters.hcut$size
fviz_dend(clusters.hcut)
fviz_cluster(clusters.hcut)

# bind the cluster value to original data
input.feature.vals <-cbind(USArrests,cluster = clusters.hcut$cluster)

# summarize the mean value of features within each cluster
input.feature.vals |> group_by(cluster) |> 
  summarise_all(mean)
```
Complete method is better. 
